---
phase: 10-error-recovery-cleanup
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [get-shit-done/workflows/execute-phase.md, get-shit-done/bin/gsd-tools.cjs]
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Specialist calls timeout after 5 minutes instead of hanging indefinitely"
    - "Failed specialist executions can be rolled back to checkpoint"
    - "System preserves partial work from timed-out specialists when salvageable"
  artifacts:
    - path: "get-shit-done/workflows/execute-phase.md"
      provides: "Timeout wrapper and checkpoint functions"
      contains: "handle_specialist_timeout|create_checkpoint|rollback_to_checkpoint"
    - path: "get-shit-done/bin/gsd-tools.cjs"
      provides: "Structured error logging command"
      contains: "log-specialist-error"
  key_links:
    - from: "execute-phase.md"
      to: "timeout command"
      via: "timeout --kill-after=10s wrapper"
      pattern: "timeout.*--kill-after.*Task\\("
    - from: "execute-phase.md"
      to: "git tag"
      via: "checkpoint creation"
      pattern: "git tag.*checkpoint/"
---

<objective>
Implement error recovery mechanisms for specialist failures including timeouts, checkpoints, and structured logging.

Purpose: Ensure system handles specialist failures gracefully without data loss or corruption
Output: Robust error recovery infrastructure in execute-phase orchestrator
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-error-recovery-cleanup/10-RESEARCH.md
@.planning/phases/08-escape-hatch-protocol/08-01-SUMMARY.md
@.planning/phases/08-escape-hatch-protocol/08-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add checkpoint management functions</name>
  <files>get-shit-done/workflows/execute-phase.md</files>
  <action>
    Add checkpoint management functions to execute-phase.md at line ~215, immediately after the validate_specialist() function (which ends at line 212):

    1. create_checkpoint() function:
       - Accept parameters: PHASE, PLAN, SPECIALIST
       - Generate timestamp: date +%Y%m%d_%H%M%S
       - Stage all changes: git add -A
       - Create commit with --no-verify flag: git commit --no-verify -m "CHECKPOINT: Before ${SPECIALIST} execution for ${PHASE}-${PLAN}"
       - Handle case where no changes to commit (use || true)
       - Create tag: git tag "checkpoint/${PHASE}-${PLAN}/${TIMESTAMP}"
       - Echo the tag name for later reference

    2. rollback_to_checkpoint() function:
       - Accept checkpoint tag as parameter
       - Verify tag exists: git tag -l "$CHECKPOINT_TAG" | grep -q .
       - If exists: git reset --hard "$CHECKPOINT_TAG"
       - Delete checkpoint tag after rollback: git tag -d "$CHECKPOINT_TAG"
       - Log rollback action to stderr
       - Handle case where tag doesn't exist (warning message)

    Follow exact patterns from 10-RESEARCH.md lines 92-120.
  </action>
  <verify>grep -E "create_checkpoint|rollback_to_checkpoint" get-shit-done/workflows/execute-phase.md | wc -l | grep -q "2"</verify>
  <done>Checkpoint management functions defined in execute-phase.md</done>
</task>

<task type="auto">
  <name>Task 2: Add timeout wrapper function</name>
  <files>get-shit-done/workflows/execute-phase.md</files>
  <action>
    Add handle_specialist_timeout() function to execute-phase.md at line ~245, after the checkpoint functions:

    1. handle_specialist_timeout() function:
       - Accept parameters: full Task() command as string, PHASE, PLAN
       - Get timeout from env var: ${SPECIALIST_TIMEOUT:-300} (5 minutes default)
       - Use GNU timeout with SIGKILL escalation:
         timeout --kill-after=10s ${TIMEOUT}s bash -c "$1"
       - Capture exit code: $?
       - Handle exit codes:
         * 124 = SIGTERM timeout (log to stderr, call gsd-tools log-specialist-error)
         * 137 = SIGKILL timeout (log to stderr, call gsd-tools log-specialist-error)
         * 0 = success (return 0)
         * Other = command failure (return exit code)
       - For timeout cases, check for partial results in output
       - Return appropriate exit code

    Implementation pattern from 10-RESEARCH.md lines 64-85, adapted for function form.

    Important: The function receives the full Task() invocation as a string that will be executed via bash -c.
  </action>
  <verify>grep -q "handle_specialist_timeout()" get-shit-done/workflows/execute-phase.md</verify>
  <done>Timeout wrapper function added to execute-phase.md</done>
</task>

<task type="auto">
  <name>Task 3: Add structured error logging command to gsd-tools</name>
  <files>get-shit-done/bin/gsd-tools.cjs</files>
  <action>
    Add a new command to gsd-tools.cjs for structured error logging:

    1. Create new command: 'log-specialist-error'
    2. Accept parameters:
       - --phase (required)
       - --plan (required)
       - --task (required)
       - --specialist (required)
       - --error-type (required, e.g., "timeout", "crash", "validation")
       - --details (required)

    3. Create JSON object with all fields plus timestamp
    4. Append to .planning/specialist-errors.jsonl (create if doesn't exist)
    5. Also call existing 'state add-error' command to update STATE.md

    This enables the execute-phase orchestrator to log structured errors that can be analyzed later.

    Implementation follows patterns from existing gsd-tools commands.
  </action>
  <verify>grep -A10 "log-specialist-error" get-shit-done/bin/gsd-tools.cjs</verify>
  <done>Structured error logging command available in gsd-tools</done>
</task>

<task type="auto">
  <name>Task 4: Integrate error recovery into specialist spawning</name>
  <files>get-shit-done/workflows/execute-phase.md</files>
  <action>
    Modify specialist spawning at two locations in execute-phase.md:

    1. Main specialist execution (around line 369):
       - Create checkpoint before Task(): CHECKPOINT_TAG=$(create_checkpoint "$PHASE" "$PLAN" "$CURRENT_SPECIALIST")
       - Wrap Task() call with timeout:
         ```bash
         SPECIALIST_RESULT=$(handle_specialist_timeout 'Task(
           subagent_type="'"${CURRENT_SPECIALIST}"'",
           model="{executor_model}",
           prompt="..."
         )' "$PHASE" "$PLAN" 2>&1)
         EXIT_CODE=$?
         ```
       - After execution, check EXIT_CODE:
         * If 0: delete checkpoint tag (git tag -d "$CHECKPOINT_TAG" 2>/dev/null)
         * If 124/137 (timeout): check for partial work, offer rollback if no salvageable work
         * Other failures: rollback_to_checkpoint "$CHECKPOINT_TAG"
       - Continue with existing parse_specialist_result() call

    2. Verification specialist spawning (around line 652):
       - Apply same pattern: checkpoint, timeout wrap, cleanup/rollback

    Note: The timeout wrapper needs the Task() call as a quoted string. Use proper quote escaping:
    'Task(subagent_type="'"${VAR}"'", ...)' to nest the variable expansion.
  </action>
  <verify>grep -B2 "handle_specialist_timeout.*Task" get-shit-done/workflows/execute-phase.md | grep -q "CHECKPOINT_TAG"</verify>
  <done>Specialist spawning integrated with checkpoint and timeout mechanisms</done>
</task>

</tasks>

<verification>
1. Execute a test specialist call with DEBUG=true and SPECIALIST_TIMEOUT=10 (10 seconds)
2. Verify timeout triggers after 10 seconds with exit code 124
3. Check that checkpoint tags are created and cleaned up on success
4. Simulate failure and verify rollback restores previous state
5. Confirm gsd-tools log-specialist-error command logs errors
</verification>

<success_criteria>
- Specialist calls timeout after configured duration instead of hanging
- Checkpoints created before risky operations enable rollback
- Structured error logging provides debugging information via gsd-tools
- System gracefully handles timeouts, crashes, and validation failures
</success_criteria>

<output>
After completion, create `.planning/phases/10-error-recovery-cleanup/10-01-SUMMARY.md`
</output>