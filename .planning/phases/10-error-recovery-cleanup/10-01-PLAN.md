---
phase: 10-error-recovery-cleanup
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [get-shit-done/workflows/execute-phase.md, get-shit-done/bin/gsd-tools.cjs]
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Specialist calls timeout after 5 minutes instead of hanging indefinitely"
    - "Failed specialist executions can be rolled back to checkpoint"
    - "System preserves partial work from timed-out specialists when salvageable"
  artifacts:
    - path: "get-shit-done/workflows/execute-phase.md"
      provides: "Timeout wrapper and checkpoint functions"
      contains: "handle_specialist_timeout|create_checkpoint|rollback_to_checkpoint"
    - path: ".planning/specialist-errors.jsonl"
      provides: "Structured error logging"
      min_lines: 1
  key_links:
    - from: "execute-phase.md"
      to: "timeout command"
      via: "timeout --kill-after=10s wrapper"
      pattern: "timeout.*--kill-after.*Task\\("
    - from: "execute-phase.md"
      to: "git tag"
      via: "checkpoint creation"
      pattern: "git tag.*checkpoint/"
---

<objective>
Implement error recovery mechanisms for specialist failures including timeouts, checkpoints, and structured logging.

Purpose: Ensure system handles specialist failures gracefully without data loss or corruption
Output: Robust error recovery infrastructure in execute-phase orchestrator
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-error-recovery-cleanup/10-RESEARCH.md
@.planning/phases/08-escape-hatch-protocol/08-01-SUMMARY.md
@.planning/phases/08-escape-hatch-protocol/08-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add timeout and checkpoint functions</name>
  <files>get-shit-done/workflows/execute-phase.md</files>
  <action>
    Add three core error recovery functions to execute-phase.md after the validate_specialist() function:

    1. handle_specialist_timeout() function:
       - Wrap Task() calls in GNU timeout with 5-minute default (configurable via SPECIALIST_TIMEOUT env var)
       - Use timeout --kill-after=10s to escalate to SIGKILL if needed
       - Capture exit code 124 for timeout, 137 for SIGKILL
       - Log structured errors to .planning/specialist-errors.jsonl
       - Return appropriate exit codes for orchestrator handling

    2. create_checkpoint() function:
       - Create git commit with all current changes (use --no-verify to skip hooks)
       - Tag with checkpoint/{phase}-{plan}/{timestamp} format
       - Return tag name for rollback reference
       - Handle case where no changes to commit (don't fail)

    3. rollback_to_checkpoint() function:
       - Accept checkpoint tag as parameter
       - Verify tag exists before attempting rollback
       - Use git reset --hard to restore state
       - Delete checkpoint tag after rollback (cleanup)
       - Log rollback action to stderr

    Implementation follows patterns from 10-RESEARCH.md lines 64-121. Use exact command syntax from research.
  </action>
  <verify>grep -E "handle_specialist_timeout|create_checkpoint|rollback_to_checkpoint" get-shit-done/workflows/execute-phase.md</verify>
  <done>All three error recovery functions defined in execute-phase.md</done>
</task>

<task type="auto">
  <name>Task 2: Integrate timeout and checkpoint into specialist spawning</name>
  <files>get-shit-done/workflows/execute-phase.md</files>
  <action>
    Modify the specialist spawning logic to use the new error recovery functions:

    1. Find the Task() invocation around line 369
    2. Replace direct Task() call with:
       - Call create_checkpoint() before spawning
       - Use handle_specialist_timeout() to wrap Task() execution
       - On failure (exit code 124 or other), offer rollback via rollback_to_checkpoint()
       - On success, delete checkpoint tag for cleanup

    3. Add similar wrapping for verification specialist spawning (around line 652)

    4. Preserve existing parse_specialist_result() call after timeout wrapper

    Pattern to follow from research (lines 228-251):
    - Create checkpoint first
    - Execute with timeout
    - Check exit code
    - Rollback on failure OR cleanup checkpoint on success

    Important: The timeout wrapper should pass the entire Task() invocation as a bash -c command string.
  </action>
  <verify>grep -B2 -A5 "handle_specialist_timeout.*SPECIALIST" get-shit-done/workflows/execute-phase.md</verify>
  <done>Task() calls wrapped with timeout and checkpoint logic</done>
</task>

<task type="auto">
  <name>Task 3: Add structured error logging support</name>
  <files>get-shit-done/bin/gsd-tools.cjs</files>
  <action>
    Add a new command to gsd-tools.cjs for structured error logging:

    1. Create new command: 'log-specialist-error'
    2. Accept parameters:
       - --phase (required)
       - --plan (required)
       - --task (required)
       - --specialist (required)
       - --error-type (required, e.g., "timeout", "crash", "validation")
       - --details (required)

    3. Create JSON object with all fields plus timestamp
    4. Append to .planning/specialist-errors.jsonl (create if doesn't exist)
    5. Also call existing 'state add-error' command to update STATE.md

    This enables the execute-phase orchestrator to log structured errors that can be analyzed later.
  </action>
  <verify>grep -A10 "log-specialist-error" get-shit-done/bin/gsd-tools.cjs</verify>
  <done>Structured error logging command available in gsd-tools</done>
</task>

</tasks>

<verification>
1. Execute a test specialist call with DEBUG=true and SPECIALIST_TIMEOUT=10 (10 seconds)
2. Verify timeout triggers after 10 seconds with exit code 124
3. Check .planning/specialist-errors.jsonl contains structured error entry
4. Verify checkpoint tag created and cleaned up on success
5. Simulate failure and verify rollback restores previous state
</verification>

<success_criteria>
- Specialist calls timeout after configured duration instead of hanging
- Checkpoints created before risky operations enable rollback
- Structured error logging provides debugging information
- System gracefully handles timeouts, crashes, and validation failures
</success_criteria>

<output>
After completion, create `.planning/phases/10-error-recovery-cleanup/10-01-SUMMARY.md`
</output>